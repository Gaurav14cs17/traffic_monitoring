{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import itertools\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find convolution of 2 shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getMask(mask_path):\n",
    "    # path to mask image\n",
    "    mask = cv2.imread(mask_path,0)\n",
    "    mask_position=set()\n",
    "    for row, value in enumerate(mask):\n",
    "        for column, element in enumerate(value):\n",
    "            if element == 255:\n",
    "                mask_position.add((column,row))\n",
    "    return mask_position\n",
    "\n",
    "def getBbox(bbox):\n",
    "    x,y,x_plus_w,y_plus_h = bbox \n",
    "    vehicle_position = itertools.product(range(x,x_plus_w+1),range(y,y_plus_h+1))\n",
    "    return set(vehicle_position)\n",
    "\n",
    "def getProbability2Shape(vehicle, mask):\n",
    "    match = len(vehicle)-len(vehicle.difference(mask))\n",
    "    bbox  = len(vehicle)\n",
    "    return (match*100)/bbox\n",
    "\n",
    "'''\n",
    "Just a mark for myself\n",
    "'''\n",
    "def get_output_layers(net):\n",
    "    '''\n",
    "    get all output layer names: with yolov3 is yolo_82, 94 and 106\n",
    "    ''' \n",
    "    layer_names = net.getLayerNames()\n",
    "    \n",
    "    output_layers = [layer_names[i[0] - 1] for i in net.getUnconnectedOutLayers()]\n",
    "\n",
    "    return output_layers\n",
    "\n",
    "\n",
    "def draw_prediction(img, class_id, confidences, x, y, x_plus_w, y_plus_h):\n",
    "    '''\n",
    "    draw a bounding box around object\n",
    "    '''\n",
    "    label = str(classes[class_id])\n",
    "\n",
    "    color = COLORS[class_id]\n",
    "    \n",
    "#     print(x,y,x_plus_w,y_plus_h)\n",
    "    cv2.rectangle(img, (x,y), (x_plus_w,y_plus_h), color, 2)\n",
    "\n",
    "    cv2.putText(img, label, (x-10,y-10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n",
    "\n",
    "def bounding_pic(net, image, scale, size,mask):\n",
    "    '''\n",
    "    net: the model reading by open-cv (maybe YoLo or SSD)\n",
    "    image: image need to bouding box\n",
    "    scale: image pixel multiply with this\n",
    "    size: a tuple contain size of image\n",
    "    '''\n",
    "    Width = image.shape[1]\n",
    "    Height = image.shape[0]\n",
    "    # Resize picture to 416x416, because YOLO take in 416x416\n",
    "    blob = cv2.dnn.blobFromImage(image, scale, (416,416), (0,0,0), True, crop=False)\n",
    "    # set input is resized picture\n",
    "    net.setInput(blob)\n",
    "    # last layer of Yolo model\n",
    "    outs = net.forward(get_output_layers(net))\n",
    "\n",
    "    class_ids = []\n",
    "    confidences = []\n",
    "    boxes = []\n",
    "    conf_threshold = 0.6\n",
    "    # maybe our model will detect many bouding box for an object, this threshold help us take the box with equal \n",
    "    #            or higher propability\n",
    "    nms_threshold = 0.4\n",
    "\n",
    "    '''\n",
    "    out is a 2D tensor like (number_of_objects, score_of_each_classes), with first five element in each row is special, \n",
    "    take e.g: out[0] = temp:\n",
    "        + temp[0]: x_center of that object\n",
    "        + temp[1]: y_center of that object\n",
    "        + temp[2]: width of that object\n",
    "        + temp[3]: height of that object\n",
    "        + temp[4]: unknow value\n",
    "        + from 5 to above is the score of that object to each classes => COCO have 80 class so each row contain 85 element,\n",
    "            will be 15 with CIFAR,and 1005 with IMAGENET  \n",
    "    '''\n",
    "    for out in outs:\n",
    "        for detection in out:\n",
    "            scores = detection[5:]\n",
    "            # get the highest score to determine its label\n",
    "            class_id = np.argmax(scores)\n",
    "            if class_id not in [0,1,2,3,7]:\n",
    "                continue\n",
    "            else:\n",
    "                # score of that object, make sure more than 50% correct label\n",
    "                confidence = scores[class_id]\n",
    "                if confidence > 0.5:\n",
    "                    # scale again with w and h\n",
    "                    center_x = int(detection[0] * Width)\n",
    "                    center_y = int(detection[1] * Height)\n",
    "                    w = int(detection[2] * Width)\n",
    "                    h = int(detection[3] * Height)\n",
    "                    # remember it return x_center and y_center, not x,y, so we need to find x,y\n",
    "                    x = center_x - w / 2\n",
    "                    y = center_y - h / 2\n",
    "                    class_ids.append(class_id)\n",
    "                    confidences.append(float(confidence))\n",
    "                    boxes.append([x, y, w, h])\n",
    "\n",
    "    # detect bouding box around objects\n",
    "    indices = cv2.dnn.NMSBoxes(boxes, confidences, conf_threshold, nms_threshold)\n",
    "    # set the counting line\n",
    "    lineThickness = 2\n",
    "    \n",
    "    for i in indices:\n",
    "        i = i[0]\n",
    "        box = boxes[i]\n",
    "        x = round(box[0])\n",
    "        y = round(box[1])\n",
    "        w = round(box[2])\n",
    "        h = round(box[3])\n",
    "        draw_prediction(image, class_ids[i], confidences[i], round(x), round(y), round(x+w), round(y+h))\n",
    "        bbox = getBbox((x,y,x+w,y+h))\n",
    "        print(len(bbox), len(mask))\n",
    "        conv = getProbability2Shape(bbox,mask)\n",
    "        text1 = 'Convolution: ' + str(round(conv,2)) + '%'\n",
    "        text2 = 'True' if conv > 80 else 'False'\n",
    "        cv2.putText(image, text1, (x, y),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 5)\n",
    "        cv2.putText(image, 'Fault: '+text2, (x, y+30),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 5)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6480 166172\n",
      "1989 166172\n",
      "3402 166172\n",
      "1026 166172\n",
      "640 166172\n",
      "924 166172\n",
      "5368 166172\n",
      "6480 166172\n",
      "2268 166172\n",
      "3264 166172\n",
      "1170 166172\n",
      "980 166172\n",
      "680 166172\n",
      "5280 166172\n",
      "6408 166172\n",
      "2184 166172\n",
      "3520 166172\n",
      "999 166172\n",
      "1036 166172\n",
      "627 166172\n",
      "5280 166172\n",
      "6408 166172\n",
      "2352 166172\n",
      "3575 166172\n",
      "1092 166172\n",
      "850 166172\n",
      "486 166172\n",
      "5280 166172\n",
      "6408 166172\n",
      "1260 166172\n",
      "3456 166172\n",
      "2337 166172\n",
      "792 166172\n",
      "504 166172\n",
      "5280 166172\n",
      "6408 166172\n",
      "3286 166172\n",
      "2419 166172\n",
      "1488 166172\n",
      "945 166172\n",
      "551 166172\n",
      "5280 166172\n",
      "6408 166172\n",
      "2646 166172\n",
      "3180 166172\n",
      "1290 166172\n",
      "918 166172\n",
      "660 166172\n",
      "5280 166172\n",
      "6408 166172\n",
      "2814 166172\n",
      "3276 166172\n",
      "1334 166172\n",
      "620 166172\n",
      "825 166172\n",
      "5280 166172\n",
      "6408 166172\n",
      "2706 166172\n",
      "3472 166172\n",
      "660 166172\n",
      "1530 166172\n",
      "800 166172\n",
      "5280 166172\n",
      "6408 166172\n",
      "3685 166172\n",
      "1872 166172\n",
      "1566 166172\n",
      "660 166172\n",
      "667 166172\n",
      "5280 166172\n",
      "6408 166172\n",
      "3819 166172\n",
      "2204 166172\n",
      "680 166172\n",
      "1530 166172\n",
      "4248 166172\n",
      "5280 166172\n",
      "2400 166172\n",
      "3795 166172\n",
      "6408 166172\n",
      "814 166172\n",
      "1537 166172\n",
      "621 166172\n",
      "6962 166172\n",
      "5280 166172\n",
      "2460 166172\n",
      "3795 166172\n",
      "6408 166172\n",
      "819 166172\n",
      "15481 166172\n",
      "1131 166172\n",
      "5280 166172\n",
      "2624 166172\n",
      "3905 166172\n",
      "6408 166172\n",
      "740 166172\n",
      "1247 166172\n",
      "18460 166172\n",
      "5280 166172\n",
      "2992 166172\n",
      "6408 166172\n",
      "760 166172\n",
      "3990 166172\n",
      "1066 166172\n",
      "23478 166172\n",
      "5280 166172\n",
      "6408 166172\n",
      "3082 166172\n",
      "3886 166172\n",
      "740 166172\n",
      "27641 166172\n",
      "1053 166172\n",
      "5280 166172\n",
      "6408 166172\n",
      "3630 166172\n",
      "3430 166172\n",
      "777 166172\n",
      "26760 166172\n",
      "1204 166172\n",
      "5280 166172\n",
      "6408 166172\n",
      "4015 166172\n",
      "26442 166172\n",
      "740 166172\n",
      "3621 166172\n",
      "1080 166172\n",
      "5280 166172\n",
      "6408 166172\n",
      "4270 166172\n",
      "777 166172\n",
      "25868 166172\n",
      "1176 166172\n",
      "3528 166172\n",
      "5429 166172\n",
      "4440 166172\n",
      "6408 166172\n",
      "1232 166172\n",
      "23690 166172\n",
      "3360 166172\n",
      "819 166172\n",
      "5429 166172\n",
      "5148 166172\n",
      "6408 166172\n",
      "1218 166172\n",
      "23484 166172\n",
      "3430 166172\n",
      "693 166172\n",
      "5429 166172\n",
      "4914 166172\n",
      "6336 166172\n",
      "1080 166172\n",
      "21960 166172\n",
      "3456 166172\n",
      "782 166172\n",
      "5368 166172\n",
      "4851 166172\n",
      "6336 166172\n",
      "1134 166172\n",
      "3869 166172\n",
      "18648 166172\n",
      "851 166172\n",
      "5368 166172\n",
      "6336 166172\n",
      "4661 166172\n",
      "1176 166172\n",
      "22200 166172\n",
      "3996 166172\n",
      "836 166172\n",
      "725 166172\n",
      "5368 166172\n"
     ]
    }
   ],
   "source": [
    "config = 'YOLOv3-416\\\\yolov3.cfg'\n",
    "name = 'YOLOv3-416\\\\coco.names'\n",
    "weight = 'YOLOv3-416\\\\yolov3.weights'\n",
    "mask_path = './mask.jpg'\n",
    "import time\n",
    "classes = None\n",
    "\n",
    "with open(name, 'r') as f:\n",
    "    # generate all classes of COCO, bicycle ind = 1, car ind = 2 and motorbike ind = 3\n",
    "    classes = [line.strip() for line in f.readlines()]\n",
    "\n",
    "COLORS = np.random.uniform(0, 255, size=(len(classes), 3))\n",
    "\n",
    "# Read the model\n",
    "net = cv2.dnn.readNet(weight, config)\n",
    "net.setPreferableBackend(cv2.dnn.DNN_BACKEND_OPENCV)\n",
    "net.setPreferableTarget(cv2.dnn.DNN_TARGET_OPENCL)\n",
    "# net.setPreferableTarget(cv2.dnn.DNN_TARGET_CPU)\n",
    "# take shape of image in order to scale it to 416x416, first layer of Yolo CNN\n",
    "\n",
    "scale = 0.00392\n",
    "\n",
    "cap = cv2.VideoCapture('Z:\\\\Vehicle speed measuring\\\\video_speed measure\\\\front.avi')\n",
    "'''\n",
    "If output is not None\n",
    "'''\n",
    "fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "outputVideo = cv2.VideoWriter('C:/Users/ADMINS/Desktop/areaResult.avi', fourcc, 20, (1920,1080))\n",
    "\n",
    "#close\n",
    "\n",
    "frameNo = 0\n",
    "count = 0\n",
    "t1 = time.time()\n",
    "\n",
    "# for session_0_center\n",
    "mask = getMask(mask_path)\n",
    "\n",
    "\n",
    "while(True):\n",
    "    # Capture frame-by-frame\n",
    "    ret, frame = cap.read()\n",
    "    thresh = 1\n",
    "    frameNo += 1\n",
    "\n",
    "    if count%thresh==0:\n",
    "        # Our operations on the frame come here\n",
    "        img = bounding_pic(net, frame, scale, (416,416),mask)\n",
    "        #510,496 1200,471 1527,616 706,749\n",
    "        \n",
    "        img = cv2.line(img, (510,496), (1200,471), (255,0,0), 3)\n",
    "        img = cv2.line(img, (1200,471), (1527,616), (255,0,0), 3)\n",
    "        img = cv2.line(img, (1527,616), (706,749), (255,0,0), 3)\n",
    "        img = cv2.line(img, (706,749), (510,496), (255,0,0), 3)\n",
    "        t2 = time.time()\n",
    "        fps = round(frameNo/(t2-t1),2)\n",
    "        img = cv2.putText(img, str(fps), (30, 30),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 5)\n",
    "        cv2.namedWindow('image',cv2.WINDOW_NORMAL)\n",
    "        cv2.resizeWindow('image', 800,600)\n",
    "        cv2.imshow('image',img)\n",
    "        outputVideo.write(frame)\n",
    "    \n",
    "    count += 1\n",
    "    if count == thresh*30:\n",
    "        count = 0\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# When everything done, release the capture\n",
    "cap.release()\n",
    "outputVideo.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find the position of vehicle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def distanceFromPoint2Line(p, line):\n",
    "    a,b,c = line[0],line[1],line[2]\n",
    "    try:\n",
    "        # if list or np.array shape 1\n",
    "        x,y = p[0],p[1]\n",
    "    except:\n",
    "        # if np.array shape 2\n",
    "        x,y = p[0][0], p[0][1]\n",
    "    finally:\n",
    "        top = abs(a*x+b*y+c)\n",
    "        bottom = np.linalg.norm(np.array([a,b]))\n",
    "        return top/bottom\n",
    "\n",
    "def cosineVetorPhase(v1,v2):\n",
    "    dotProduct = np.dot(v1,v2)\n",
    "    normV1 = np.linalg.norm(v1)\n",
    "    normV2 = np.linalg.norm(v2)\n",
    "    return dotProduct/(normV1*normV2)\n",
    "\n",
    "def checkSameSideNormalVector(p, line):\n",
    "    # p is numpy array with shape [1,3] with the final element is 1\n",
    "    # line is something like ax+by+c = 0, line =[a,b,c]\n",
    "    return (line[0]*p[0]+line[1]*p[1]+c) > 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SYSTEM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras import backend as K\n",
    "from keras.models import load_model\n",
    "from keras.layers import Input\n",
    "from PIL import Image, ImageFont, ImageDraw\n",
    "from timeit import default_timer as timer\n",
    "from keras.utils import multi_gpu_model\n",
    "from  yolo_loading import *\n",
    "\n",
    "import tensorflow.compat.v1 as tensorflow\n",
    "import colorsys\n",
    "import os\n",
    "import utilities\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class Yolo(object):\n",
    "#     from sort import Sort\n",
    "    _defaults = {\n",
    "        \"model_path\": 'YOLOv3-416/yolo.h5',\n",
    "        \"anchors_path\": 'YOLOv3-416/yolo_anchors.txt',\n",
    "        \"classes_path\": 'YOLOv3-416/coco.names',\n",
    "        \"model_image_size\" : (416, 416),\n",
    "        \"gpu_num\" : 1,\n",
    "    }\n",
    "\n",
    "    @classmethod\n",
    "    def get_defaults(cls, n):\n",
    "        if n in cls._defaults:\n",
    "            return cls._defaults[n]\n",
    "        else:\n",
    "            return \"Unrecognized attribute name '\" + n + \"'\"\n",
    "\n",
    "    def __init__(self, score, iou, **kwargs):\n",
    "        self.__dict__.update(self._defaults) # set up default values\n",
    "        self.__dict__.update(kwargs) # and update with user overrides\n",
    "        self.class_names = self._get_class()\n",
    "        self.score = score\n",
    "        self.iou   = iou\n",
    "        self.anchors = self._get_anchors()\n",
    "        self.sess = K.get_session()\n",
    "        self.boxes, self.scores, self.classes = self.generate()\n",
    "#         self.tracker=Sort()\n",
    "    def _get_class(self):\n",
    "        classes_path = os.path.expanduser(self.classes_path)\n",
    "        with open(classes_path) as f:\n",
    "            class_names = f.readlines()\n",
    "        class_names = [c.strip() for c in class_names]\n",
    "        return class_names\n",
    "\n",
    "    def _get_anchors(self):\n",
    "        anchors_path = os.path.expanduser(self.anchors_path)\n",
    "        with open(anchors_path) as f:\n",
    "            anchors = f.readline()\n",
    "        anchors = [float(x) for x in anchors.split(',')]\n",
    "        return np.array(anchors).reshape(-1, 2)\n",
    "\n",
    "    def generate(self):\n",
    "        model_path = os.path.expanduser(self.model_path)\n",
    "        assert model_path.endswith('.h5'), 'Keras model or weights must be a .h5 file.'\n",
    "\n",
    "        # Load model, or construct model and load weights.\n",
    "        num_anchors = len(self.anchors)\n",
    "        num_classes = len(self.class_names)\n",
    "        is_tiny_version = num_anchors==6 # default setting\n",
    "        try:\n",
    "            self.yolo_model = load_model(model_path, compile=False)\n",
    "        except:\n",
    "            self.yolo_model = tiny_yolo_body(Input(shape=(None,None,3)), num_anchors//2, num_classes) \\\n",
    "                if is_tiny_version else yolo_body(Input(shape=(None,None,3)), num_anchors//3, num_classes)\n",
    "            self.yolo_model.load_weights(self.model_path) # make sure model, anchors and classes match\n",
    "        else:\n",
    "            assert self.yolo_model.layers[-1].output_shape[-1] == \\\n",
    "                num_anchors/len(self.yolo_model.output) * (num_classes + 5), \\\n",
    "                'Mismatch between model and given anchor and class sizes'\n",
    "\n",
    "        print('{} model, anchors, and classes loaded.'.format(model_path))\n",
    "\n",
    "        # Generate colors for drawing bounding boxes.\n",
    "        hsv_tuples = [(x / len(self.class_names), 1., 1.)\n",
    "                      for x in range(len(self.class_names))]\n",
    "        self.colors = list(map(lambda x: colorsys.hsv_to_rgb(*x), hsv_tuples))\n",
    "        self.colors = list(\n",
    "            map(lambda x: (int(x[0] * 255), int(x[1] * 255), int(x[2] * 255)),\n",
    "                self.colors))\n",
    "        np.random.seed(10101)  # Fixed seed for consistent colors across runs.\n",
    "        np.random.shuffle(self.colors)  # Shuffle colors to decorrelate adjacent classes.\n",
    "        np.random.seed(None)  # Reset seed to default.\n",
    "\n",
    "        # Generate output tensor targets for filtered bounding boxes.\n",
    "        self.input_image_shape = K.placeholder(shape=(2, ))\n",
    "        if self.gpu_num>=2:\n",
    "            self.yolo_model = multi_gpu_model(self.yolo_model, gpus=self.gpu_num)\n",
    "        boxes, scores, classes = yolo_eval(self.yolo_model.output, self.anchors,\n",
    "                len(self.class_names), self.input_image_shape,\n",
    "                score_threshold=self.score, iou_threshold=self.iou)\n",
    "        return boxes, scores, classes\n",
    "\n",
    "    def close_session(self):\n",
    "        self.sess.close()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Debug for cross red line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import itertools\n",
    "\n",
    "def getMask(mask_path):\n",
    "    # path to mask image\n",
    "    mask = cv2.imread(mask_path,0)\n",
    "    mask_position=set()\n",
    "    for row, value in enumerate(mask):\n",
    "        for column, element in enumerate(value):\n",
    "            if element == 255:\n",
    "                mask_position.add((column,row))\n",
    "    return mask_position\n",
    "\n",
    "def getBbox(bbox):\n",
    "    x,y,x_plus_w,y_plus_h = bbox \n",
    "    vehicle_position = itertools.product(range(x,x_plus_w+1),range(y,y_plus_h+1))\n",
    "    return set(vehicle_position)\n",
    "\n",
    "def getProbability2Shape(vehicle, mask):\n",
    "    match = len(vehicle)-len(vehicle.difference(mask))\n",
    "    bbox  = len(vehicle)\n",
    "    return (match*100)/bbox\n",
    "\n",
    "def distanceFromPoint2Line(p, line):\n",
    "    a,b,c = line[0],line[1],line[2]\n",
    "    try:\n",
    "        # if list or np.array shape 1\n",
    "        x,y = p[0],p[1]\n",
    "    except:\n",
    "        # if np.array shape 2\n",
    "        x,y = p[0][0], p[0][1]\n",
    "    finally:\n",
    "        top = abs(a*x+b*y+c)\n",
    "        bottom = np.linalg.norm(np.array([a,b]))\n",
    "        return top/bottom\n",
    "\n",
    "def cosineVectorPhase(v1,v2):\n",
    "    dotProduct = np.dot(v1,v2)\n",
    "    normV1 = np.linalg.norm(v1)\n",
    "    normV2 = np.linalg.norm(v2)\n",
    "    return dotProduct/(normV1*normV2)\n",
    "\n",
    "def checkFromTop(p, p_line1, p_line2):\n",
    "    x,y = p\n",
    "    x1 = p_line1[0]\n",
    "    y1 = p_line1[1]\n",
    "    x2 = p_line2[0]\n",
    "    y2 = p_line2[1]\n",
    "    return (x-x1)*(y2-y1)-(y-y1)*(x2-x1) > 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# vehicle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sort import *\n",
    "from utils import *\n",
    "# from vehicle import *\n",
    "# from cross_red_line import *\n",
    "\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.models import load_model\n",
    "from keras.layers import Input\n",
    "from PIL import Image, ImageFont, ImageDraw\n",
    "\n",
    "import os\n",
    "import utilities\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "import time\n",
    "\n",
    "def bbox2necess(image, bbox,frame,shape):\n",
    "    \"\"\"\n",
    "    return a list, each element is a list contain [posX,posY,ID,frame_no, image bbox]\n",
    "    \"\"\"\n",
    "    final_res=[]\n",
    "    width = shape[0]\n",
    "    height = shape[1]\n",
    "#     print(width,height)\n",
    "    for box in bbox:\n",
    "#         print(box)\n",
    "        x = int(round(box[0]))\n",
    "        y = int(round(box[1]))\n",
    "        w = int(round(box[2]-box[0]))\n",
    "        h = int(round(box[3]))-int(round(box[1]))\n",
    "#         print(x,y,w,h)\n",
    "        x_plus_w = x + w\n",
    "        y_plus_h = y + h\n",
    "        bbox2d = image[x:x_plus_w,y:y_plus_h]\n",
    "        x_centroid = x + w/2\n",
    "        y_centroid = y + h/2\n",
    "        bbox2d_position = (y ,x, y_plus_h, x_plus_w)\n",
    "        res=[x_centroid,y_centroid,(box[4]), frame, bbox2d, bbox2d_position]\n",
    "        final_res.append(res)\n",
    "    return final_res\n",
    "\n",
    "\n",
    "def detect_video(yolo, video_type, video_path, output_path, mask_path, mode, \n",
    "                    scale, vp1, vp2, pp, allow_speed, best_performance_line,\n",
    "                    deadline4Red, allow_lanes, all_lanes, thresh_frame):\n",
    "    '''\n",
    "    - Input:\n",
    "        + yolo: yolo model\n",
    "        + video_type: 'local' when use video in PC and 'stream' for....stream\n",
    "        + video_path: path of video\n",
    "        + output_path: write down if u wanna something more clearly\n",
    "    - Output:\n",
    "        + A tensor [x,y,ID,frame_num]\n",
    "    '''\n",
    "    # these thing should append into data file\n",
    "    tuple_cam = computeCameraCalibration(vp1,vp2,pp)\n",
    "    t1 = time.time()\n",
    "    # show for debug\n",
    "    \n",
    "#     cv2.namedWindow('image',cv2.WINDOW_NORMAL)\n",
    "#     cv2.resizeWindow('image', 1000,600)\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # require opencv 3.2\n",
    "    \n",
    "    vid = cv2.VideoCapture(video_path)\n",
    "    if not vid.isOpened():\n",
    "        raise IOError(\"Couldn't open webcam or video\")\n",
    "    if video_type == 'stream':\n",
    "        fps = vid.get(cv2.CAP_PROP_FPS)\n",
    "    elif video_type == 'local':\n",
    "        fps = vid.get(5)\n",
    "    # get(7) is all number of frame in video\n",
    "    # get(5) is get fps, fuck\n",
    "    \n",
    "    video_FourCC = cv2.VideoWriter_fourcc('M','J','P','G')\n",
    "    video_size      = (int(vid.get(cv2.CAP_PROP_FRAME_WIDTH)),\n",
    "                        int(vid.get(cv2.CAP_PROP_FRAME_HEIGHT)))\n",
    "#     print(video_size)\n",
    "    isOutput = True if output_path != \"\" else False\n",
    "    if isOutput:\n",
    "#         print(\"!!! TYPE:\", type(output_path), type(video_FourCC), type(fps), type(video_size))\n",
    "        out = cv2.VideoWriter(output_path, video_FourCC, fps, video_size, 1)\n",
    "\n",
    "    tracker=Sort()\n",
    "    frame_num=0\n",
    "    all_vehicle={}\n",
    "    ignore_set = set()\n",
    "\n",
    "    mask = getMask(mask_path)\n",
    "    # print(mask)\n",
    "    while True:\n",
    "        return_value, pic = vid.read()\n",
    "        if not return_value:\n",
    "            break\n",
    "        image = Image.fromarray(pic)\n",
    "        if frame_num%thresh_frame==0:\n",
    "            if yolo.model_image_size != (None, None):\n",
    "                assert yolo.model_image_size[0]%32 == 0, 'Multiples of 32 required'\n",
    "                assert yolo.model_image_size[1]%32 == 0, 'Multiples of 32 required'\n",
    "                boxed_image = letterbox_image(image, tuple(reversed(yolo.model_image_size)))\n",
    "            else:\n",
    "                new_image_size = (image.width - (image.width % 32),\n",
    "                                image.height - (image.height % 32))\n",
    "                boxed_image = letterbox_image(image, new_image_size)\n",
    "            image_data = np.array(boxed_image, dtype='float32')\n",
    "            image_data /= 255.\n",
    "            image_data = np.expand_dims(image_data, 0)  # Add batch dimension.\n",
    "            \n",
    "            # detect for bbox right here\n",
    "\n",
    "            out_boxes, out_scores, out_classes = yolo.sess.run( \n",
    "                [yolo.boxes, yolo.scores, yolo.classes],  \n",
    "                feed_dict={\n",
    "                    yolo.yolo_model.input: image_data,\n",
    "                    yolo.input_image_shape: [image.size[1], image.size[0]],\n",
    "                    K.learning_phase(): 0\n",
    "                })\n",
    "            final_box=[]\n",
    "            label=[]\n",
    "            scores=[]\n",
    "            for b,lb,sc in zip(out_boxes,out_classes,out_scores):\n",
    "                if lb in [2,3,5,7]:\n",
    "                    final_box.append(b)\n",
    "                    label.append(lb)\n",
    "                    scores.append(sc)\n",
    "\n",
    "            out_boxes=np.array(final_box)\n",
    "            out_classes=np.array(label)\n",
    "            out_scores=np.array(scores)\n",
    "            final_boxes=np.column_stack((out_boxes,out_scores))\n",
    "            final_boxes = final_boxes[np.logical_and(final_boxes[:, 4] > 0.3, final_boxes[:, 2] -\n",
    "                                                        final_boxes[:, 0] < 600)]\n",
    "\n",
    "                # Apply NMS\n",
    "            indices = utilities.non_max_suppression(final_boxes, 0.9, final_boxes[:, 4])\n",
    "            \n",
    "            out_boxes = [final_boxes[i] for i in indices]\n",
    "            out_classes= [out_classes[i] for i in indices]\n",
    "            rev=(reversed(out_classes))  # for display in order since yolo reverse the list \n",
    "            out_classes=[]\n",
    "            for r in rev:\n",
    "                out_classes.append(r)\n",
    "            out_classes=np.array(out_classes)\n",
    "            bf=out_boxes\n",
    "            res_track=tracker.update(np.array(out_boxes))\n",
    "            # res_track return [x,y, x+w, x+y, ID]\n",
    "            one_frame = bbox2necess(image = pic, bbox = res_track,frame =frame_num,\n",
    "                                        shape = video_size)\n",
    "            for vehicle in one_frame:\n",
    "                ID =  int(round(vehicle[2]))\n",
    "                centroid = [vehicle[0],vehicle[1]]\n",
    "                frame_appear = frame_num\n",
    "                bbox = vehicle[4]\n",
    "                bbox2d_position = vehicle[5]\n",
    "\n",
    "                x,y,x_plus_w,y_plus_h = bbox2d_position\n",
    "#                 print(bbox2d_position)\n",
    "#                 image = cv2.circle(pic, centroid, 5, [0,255,255], 3)\n",
    "                cv2.rectangle(pic, (x,y), (x_plus_w ,y_plus_h), (255,255,0), 2)                \n",
    "                # for show video, will delete later\n",
    "                font                   = cv2.FONT_HERSHEY_SIMPLEX\n",
    "                fontScale              = 1\n",
    "                fontColor              = (0,0,255)\n",
    "                thickness              = 3\n",
    "                linetype               = cv2.LINE_AA\n",
    "                c_0 = int(round(centroid[0]))\n",
    "                c_1 = int(round(centroid[1]))\n",
    "                #end \n",
    "                traffic_status = 'red'\n",
    "                \n",
    "                cv2.putText(pic,str(ID), \n",
    "                                (c_1 - 10 , c_0 -10), \n",
    "                                font, \n",
    "                                fontScale,\n",
    "                                fontColor,\n",
    "                                thickness,\n",
    "                                linetype)\n",
    "#                 image = cv2.circle(pic, (c_1,c_0), 5, [0,255,255], 3)\n",
    "                if ID in ignore_set:\n",
    "                    continue\n",
    "                    \n",
    "                if ID not in all_vehicle.keys():\n",
    "                    all_vehicle[ID] = Vehicle(ID, centroid, frame_appear, bbox, allow_lanes, \n",
    "                                                all_lanes, mode = mode)\n",
    "                    if mode == 'speed':\n",
    "                        all_vehicle[ID].setParemeter4speedMeasure(fps, scale, tuple_cam,\n",
    "                                                            allow_speed, best_performance_line)\n",
    "                    elif mode == 'crossRedLine':\n",
    "                        if not checkFromTop(centroid, [507,498],[1199,472]):\n",
    "                            print(\"ID: {}\".format(ID))\n",
    "                            ignore_set.add(ID)\n",
    "                        all_vehicle[ID].setParemeter4crossRedLine(deadline = deadline4Red, \n",
    "                                            traffic_status = traffic_status)\n",
    "                    continue\n",
    "\n",
    "                if mode =='speed':\n",
    "                    all_vehicle[ID].update_for_highway(bbox, centroid, frame_appear)\n",
    "                    #show for debug\n",
    "\n",
    "                    cv2.putText(pic,str(all_vehicle[ID].speed), \n",
    "                                (c_1 -10 , c_0 +20), \n",
    "                                font, \n",
    "                                fontScale,\n",
    "                                fontColor,\n",
    "                                thickness,\n",
    "                                linetype)\n",
    "#                     out.write(pic)\n",
    "#                     cv2.imshow('image',pic)\n",
    "\n",
    "                elif mode == 'crossRedLine':\n",
    "                    cv2.putText(pic, all_vehicle[ID].catched, \n",
    "                                (c_1 - 10 , c_0 +20), \n",
    "                                font, \n",
    "                                fontScale,\n",
    "                                fontColor,\n",
    "                                thickness,\n",
    "                                linetype)\n",
    "#                     out.write(pic)\n",
    "#                     cv2.imshow('image',pic)\n",
    "                    t2 = time.time()\n",
    "                    fps_temp = round(frame_num/(t2-t1),2)\n",
    "                    img = cv2.putText(pic, \"fps: \"+str(fps_temp), (30, 30),\n",
    "                                    cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 5)\n",
    "                    all_vehicle[ID].update_for_cross_redline(centroid, frame_appear, \n",
    "                                    traffic_status, bbox2d_position, mask)\n",
    "    \n",
    "\n",
    "#         if cv2.waitKey(25) & 0xFF == ord('q'):\n",
    "#             break  \n",
    "        out.write(pic)\n",
    "#         print(frame_num)\n",
    "        frame_num+=1\n",
    "        \n",
    "    vid.release()\n",
    "    out.release()\n",
    "    cv2.destroyAllWindows()\n",
    "KalmanBoxTracker.count = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "need to fix:\n",
    " + change cross lane permission into parameter.\n",
    " + set time disappear to clean up while running, for higher speed and for rtsp.\n",
    " + cross red line, deadline 28/10/2019.\n",
    " + build a basic model to recognize color in traffic light area.\n",
    "finish:\n",
    " + debug for speed.\n",
    " + debug for fps.\n",
    " + debug for saving path of fault vehicle.\n",
    " + debug for cross Lane.\n",
    " + devide 2 part for this class, private update for each situation.\n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "from vehicle_speed import *\n",
    "# from cross_red_line import *\n",
    "\n",
    "class Vehicle:\n",
    "    def __init__(self, ID, centroid, frame_appear,  bbox, \n",
    "                     allow_lanes, all_lanes, **kwags):\n",
    "        '''\n",
    "        watch ignore list in tracking\n",
    "        tuple_cam include vp1,vp2, vp3, pp, roadPlane, focal (put this into main)\n",
    "        calculate speed, measure lane, cross line if fault, save bbox into directionary\n",
    "        delete vehicle in N time disappear\n",
    "        check if thresh car problem in 1 time -> traffic jam\n",
    "        # fix to have bbox in main #\n",
    "        '''\n",
    "        '''\n",
    "        for cross_line:\n",
    "            accept_line\n",
    "            right_ditection = False\n",
    "            area of interest\n",
    "            \n",
    "        '''\n",
    "        #### need another network to regconize the license plate for both speed and\n",
    "        #### cross red line (especially this)\n",
    "        self.ID = ID\n",
    "        self.centroids = [centroid, None]\n",
    "        self.frame = [frame_appear, None]\n",
    "        self.bbox  = bbox # image\n",
    "        self.lane = getLaneForPoint(centroid,all_lanes)\n",
    "\n",
    "        # this thing\n",
    "        self._catch_cross_lane = False\n",
    "        \n",
    "        \n",
    "        self.allow_lanes = allow_lanes\n",
    "        self.all_lanes = all_lanes\n",
    "        \n",
    "        # this part 's belong to **kwags, should not change it\n",
    "        self._called = 0\n",
    "        # Use for save the speed of fault car\n",
    "        self.mode = 'speed'\n",
    "        self._overSpeed_path = './OverSpeed/'\n",
    "        self._crossLane_path = './CrossLane/'\n",
    "        self._crossRedLine_path = './crossRedLine/'\n",
    "        self._problem_path = './carWithProblem/'\n",
    "        self._crossLane = False\n",
    "        self._problem   = False\n",
    "\n",
    "        if self._catch_cross_lane and (self.lane not in self.allow_lanes) and not self._crossLane:\n",
    "            self.catch_fault_vehicle(self._crossLane_path)\n",
    "            self._crossLane = True\n",
    "\n",
    "    def setParemeter4speedMeasure(self, fps, scale, tuple_cam, allow_speed, \n",
    "                                    best_performance_line):\n",
    "        # set parameter if mode is speed\n",
    "        self.speed = 0\n",
    "        self.fps = fps\n",
    "        self.scale = scale\n",
    "        self.allow_speed = allow_speed\n",
    "        self.speed_avarage = []\n",
    "        self.tuple_cam = tuple_cam\n",
    "        self.best_performance_line = best_performance_line\n",
    "        self._overSpeed = False\n",
    "\n",
    "        \n",
    "    def update_for_highway(self, new_bbox, new_centroid, new_frame):\n",
    "        # update all element and calculate speed, instead of all the other fault\n",
    "        # except cross line\n",
    "        # this function update in mode 'highway' in main, can measure speed and \n",
    "        # detect the vehicle with problem\n",
    "        # Assume that there will be no traffic jam on highway\n",
    "\n",
    "\n",
    "        vp1, vp2, vp3, pp, roadPlane, focal = self.tuple_cam\n",
    "        laneDivLines = self.all_lanes\n",
    "        self.centroids[1] = new_centroid\n",
    "        self.frame[1] = new_frame\n",
    "        self.bbox = new_bbox\n",
    "        self.lane = getLaneForPoint(new_centroid, laneDivLines)\n",
    "\n",
    "        self._called += 1\n",
    "        time_appear = self._called/self.fps\n",
    "        \n",
    "        # On highway, which car appears more than 20 second and doesn't move\n",
    "        # => gets in trouble\n",
    "        if time_appear >= 20 and np.average(np.array(self.speed)) <= 5 and not self._problem:\n",
    "            self._problem = True\n",
    "            self.catch_fault_vehicle(self._problem_path)\n",
    "\n",
    "        # On highway, lane is count from left to right, start at 1, only catch \n",
    "        # fault if this road is not allow to cross lane.\n",
    "        if self._catch_cross_lane and (self.lane not in self.allow_lanes) and not self._crossLane:\n",
    "            self.catch_fault_vehicle(self._crossLane_path)\n",
    "            self._crossLane = True\n",
    "\n",
    "        # Make sure car in the area with best camera calibration for best measurement\n",
    "        # take avarage speed for problem car\n",
    "        ### lack of counting time to clean up whenever a car disappear for 3s\n",
    "        if  (not(getLaneForPoint(self.centroids[0], laneDivLines) is None or \\\n",
    "               getLaneForPoint(self.centroids[1], laneDivLines) is None)) \\\n",
    "               and new_centroid[0]>= self.best_performance_line:\n",
    "            frame_diff = self.frame[1]-self.frame[0]\n",
    "            \n",
    "            self.speed = calculateSpeeds(self.centroids[0],self.centroids[1], self.fps, \n",
    "                                self.scale, frame_diff, self.tuple_cam)\n",
    "            self.speed_avarage.append(self.speed)\n",
    "\n",
    "            if (self.speed > self.allow_speed) and not self._overSpeed :\n",
    "                self.catch_fault_vehicle(self._overSpeed_path)\n",
    "                self._overSpeed = True\n",
    "        self.centroids[0] = new_centroid\n",
    "        self.frame[0] = new_frame\n",
    "\n",
    "    def setParemeter4crossRedLine(self, deadline, traffic_status):\n",
    "        self._crossRedLine = False\n",
    "        self._right_direction = False\n",
    "        self.deadline = deadline\n",
    "        self.traffic_status = traffic_status\n",
    "        \n",
    "        # rediculous\n",
    "\n",
    "        self.catched = \"Nope\"\n",
    "        \n",
    "\n",
    "    def update_for_cross_redline(self, new_centroid, frame_appear, traffic_status,\n",
    "                                 bbox2D_position, mask):\n",
    "        self.centroids[1] = new_centroid\n",
    "        \n",
    "#         a = distanceFromPoint2Line(new_centroid, self.deadline)\n",
    "#         print(\"ID: {}, dist: {}\".format(self.ID, a))\n",
    "        \n",
    "#         if distanceFromPoint2Line(new_centroid, self.deadline) <= 300:\n",
    "        v = np.array([self.centroids[1][0]-self.centroids[0][0], \\\n",
    "                    self.centroids[1][1]-self.centroids[0][0]])\n",
    "        n = np.array([self.deadline[0],self.deadline[1]])\n",
    "        cosine_phase = cosineVectorPhase(v,n)\n",
    "#             print(\"ID: {}, cosine: {}\".format(self.ID,cosine_phase))    \n",
    "        if cosine_phase < 1 and cosine_phase > 0:\n",
    "            self._right_direction = True\n",
    "        \n",
    "        #[507,498],[1199,472] from GIMP\n",
    "#         if self.ID ==1:\n",
    "#             print(new_centroid)\n",
    "#             print(checkFromTop(new_centroid, [507,498],[1199,472]))\n",
    "        if self._right_direction and (self.traffic_status == 'red') and (not checkFromTop(new_centroid, [507,498],[1199,472])):\n",
    "            vehicle = getBbox(bbox2D_position)\n",
    "#                 print(vehicle)\n",
    "            prob = round(getProbability2Shape(vehicle, mask),2)\n",
    "            if self.ID == 1:\n",
    "                print(\"ID: {}, prop: {}, position: {}\".format(self.ID, prob, new_centroid[0]))\n",
    "#                 print(type(vehicle),type(mask))\n",
    "#                 print(len(vehicle), len(mask))\n",
    "\n",
    "            if prob >= 80:\n",
    "                self.catch_fault_vehicle(self._crossRedLine_path)\n",
    "\n",
    "                self.catched = \"yeah\"\n",
    "\n",
    "        self.centroids[0]=new_centroid\n",
    "\n",
    "    def catch_fault_vehicle(self, src):\n",
    "        # mode: 'speed', 'other'\n",
    "        ID, frame, bbox , mode = self.ID, self.frame[-1], self.bbox, self.mode\n",
    "#         if self.mode == 'speed':\n",
    "#             cv2.imwrite(src + str(frame) + '_' + str(round(self.speed,2))+ \"_vehicle_\" + str(ID) + \".jpg\", bbox)\n",
    "#         else:\n",
    "        cv2.imwrite(src+str(frame)+ \"_vehicle_\" + str(ID) + \".jpg\", bbox)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID: 6\n",
      "ID: 5\n",
      "ID: 7\n",
      "ID: 9\n",
      "ID: 10\n",
      "ID: 11\n",
      "ID: 1, prop: 0.0, position: 373.0\n",
      "ID: 1, prop: 0.0, position: 378.5\n",
      "ID: 1, prop: 0.0, position: 378.0\n",
      "ID: 1, prop: 0.0, position: 378.5\n",
      "ID: 1, prop: 0.0, position: 385.5\n",
      "ID: 1, prop: 0.0, position: 388.5\n",
      "ID: 1, prop: 0.0, position: 395.5\n",
      "ID: 1, prop: 0.0, position: 399.5\n",
      "ID: 1, prop: 0.0, position: 405.0\n",
      "ID: 1, prop: 0.0, position: 409.0\n",
      "ID: 1, prop: 0.0, position: 411.5\n",
      "ID: 1, prop: 0.0, position: 415.5\n",
      "ID: 1, prop: 0.0, position: 423.0\n",
      "ID: 1, prop: 0.0, position: 426.5\n",
      "ID: 1, prop: 0.0, position: 434.0\n",
      "ID: 1, prop: 0.0, position: 440.5\n",
      "ID: 1, prop: 0.0, position: 446.0\n",
      "ID: 1, prop: 0.0, position: 450.0\n",
      "ID: 1, prop: 1.37, position: 454.0\n",
      "ID: 1, prop: 9.22, position: 461.0\n",
      "ID: 1, prop: 17.46, position: 467.0\n",
      "ID: 1, prop: 27.09, position: 474.5\n",
      "ID: 1, prop: 32.97, position: 479.0\n",
      "ID: 1, prop: 37.08, position: 482.5\n",
      "ID: 1, prop: 41.76, position: 486.5\n",
      "ID: 1, prop: 50.82, position: 495.0\n",
      "ID: 1, prop: 60.54, position: 504.5\n",
      "ID: 1, prop: 67.34, position: 511.5\n",
      "ID: 1, prop: 71.92, position: 516.5\n",
      "ID: 1, prop: 77.33, position: 522.5\n",
      "ID: 1, prop: 86.17, position: 532.0\n",
      "ID: 1, prop: 89.62, position: 537.5\n",
      "ID: 1, prop: 97.64, position: 547.5\n",
      "ID: 1, prop: 100.0, position: 555.0\n",
      "ID: 1, prop: 100.0, position: 565.0\n",
      "ID: 1, prop: 100.0, position: 576.5\n",
      "ID: 1, prop: 100.0, position: 585.5\n",
      "ID: 1, prop: 100.0, position: 592.5\n",
      "ID: 1, prop: 100.0, position: 602.0\n",
      "ID: 1, prop: 100.0, position: 610.0\n",
      "ID: 1, prop: 100.0, position: 623.0\n",
      "ID: 1, prop: 100.0, position: 636.0\n",
      "ID: 1, prop: 100.0, position: 646.5\n",
      "ID: 1, prop: 97.77, position: 656.5\n",
      "ID: 1, prop: 87.59, position: 671.0\n",
      "ID: 1, prop: 79.31, position: 680.5\n",
      "ID: 1, prop: 66.41, position: 698.0\n",
      "ID: 1, prop: 55.04, position: 713.5\n",
      "ID: 1, prop: 44.2, position: 729.0\n",
      "ID: 1, prop: 33.54, position: 746.0\n",
      "ID: 1, prop: 26.04, position: 758.0\n",
      "ID: 1, prop: 13.98, position: 778.0\n",
      "ID: 1, prop: 4.78, position: 795.0\n",
      "ID: 1, prop: 0.18, position: 817.0\n",
      "ID: 16\n",
      "ID: 1, prop: 0.0, position: 838.5\n",
      "ID: 1, prop: 0.0, position: 859.5\n",
      "ID: 1, prop: 0.0, position: 882.5\n",
      "ID: 1, prop: 0.0, position: 902.5\n",
      "ID: 1, prop: 0.0, position: 931.0\n",
      "ID: 1, prop: 0.0, position: 952.0\n",
      "ID: 1, prop: 0.0, position: 962.0\n",
      "ID: 1, prop: 0.0, position: 983.5\n",
      "ID: 1, prop: 0.0, position: 987.0\n",
      "ID: 1, prop: 0.0, position: 1002.0\n",
      "ID: 1, prop: 0.0, position: 1020.0\n",
      "ID: 1, prop: 0.0, position: 1040.5\n",
      "ID: 20\n",
      "ID: 22\n",
      "ID: 21\n",
      "ID: 23\n",
      "ID: 25\n",
      "ID: 26\n",
      "ID: 28\n",
      "ID: 29\n",
      "ID: 30\n",
      "ID: 31\n",
      "ID: 35\n",
      "ID: 37\n",
      "ID: 39\n",
      "ID: 40\n",
      "ID: 43\n",
      "ID: 44\n",
      "ID: 46\n",
      "ID: 48\n",
      "ID: 50\n",
      "ID: 51\n",
      "ID: 52\n",
      "ID: 56\n",
      "ID: 57\n",
      "ID: 58\n",
      "ID: 59\n",
      "ID: 63\n",
      "ID: 65\n",
      "ID: 67\n",
      "ID: 68\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-45-f1a9fb33c169>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m                 \u001b[0mallow_speed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m40\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbest_performance_line\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m350\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdeadline4Red\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdeadline\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m                 \u001b[0mallow_lanes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mallow_lanes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mall_lanes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mall_lanes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m                 thresh_frame = 1)\n\u001b[0m",
      "\u001b[1;32m<ipython-input-42-398334d6aa9a>\u001b[0m in \u001b[0;36mdetect_video\u001b[1;34m(yolo, video_type, video_path, output_path, mask_path, mode, scale, vp1, vp2, pp, allow_speed, best_performance_line, deadline4Red, allow_lanes, all_lanes, thresh_frame)\u001b[0m\n\u001b[0;32m    120\u001b[0m                     \u001b[0myolo\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0myolo_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mimage_data\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    121\u001b[0m                     \u001b[0myolo\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minput_image_shape\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 122\u001b[1;33m                     \u001b[0mK\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearning_phase\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    123\u001b[0m                 })\n\u001b[0;32m    124\u001b[0m             \u001b[0mfinal_box\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    948\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    949\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 950\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    951\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    952\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1171\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1172\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1173\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1174\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1175\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1348\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1349\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1350\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1351\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1352\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1354\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1355\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1356\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1357\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1358\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1339\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1340\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m-> 1341\u001b[1;33m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1342\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1343\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1427\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[0;32m   1428\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1429\u001b[1;33m         run_metadata)\n\u001b[0m\u001b[0;32m   1430\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1431\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "KalmanBoxTracker.count = 0\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "video_path = 'Z:\\\\Vehicle speed measuring\\\\video_speed measure\\\\front.avi'\n",
    "# yolo = Yolo(score = 0.3, iou = 0.45)\n",
    "vp1 = [144.737, 34.7794]\n",
    "vp2 = [12183.582175112755, 615.451021479187]\n",
    "pp = [960, 540]\n",
    "scale = 0.01822590999670784\n",
    "all_lanes = [np.array([ -0.92226493,   0.38655841, 130.89136422]),\n",
    "             np.array([-0.78325241,  0.62170384, 97.52101372]),\n",
    "             np.array([-0.65281474,  0.7575176 , 71.40300615]), \n",
    "             np.array([  0.57,  -1.  , -47.39])]\n",
    "allow_lanes = [1,2]\n",
    "deadline = [-0.041, +1, +518.585]\n",
    "a = detect_video(yolo = yolo, video_type = 'local', video_path = video_path, output_path = \"C:/Users/ADMINS/Desktop/tracking_red.avi\",\n",
    "                mask_path= 'mask.jpg', mode= 'crossRedLine', scale = scale,vp1 = vp1, vp2 = vp2, pp = pp, \n",
    "                allow_speed = 40, best_performance_line = 350, deadline4Red = deadline,\n",
    "                allow_lanes = allow_lanes, all_lanes = all_lanes,\n",
    "                thresh_frame = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.14.0'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow\n",
    "tensorflow.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "incompatible dimensions for cross product\n(dimension must be 2 or 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-55-526c2d543154>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtuple_cam\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcomputeCameraCalibration\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvp1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvp2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mvp1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvp2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvp3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mroadPlane\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfocal\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtuple_cam\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mprojector\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mgetWorldCoordinagesOnRoadPlane\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfocal\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mroadPlane\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mpoints\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m6\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m7\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mpoints\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprojector\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpoints\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-43-630eb85be5f3>\u001b[0m in \u001b[0;36mcomputeCameraCalibration\u001b[1;34m(_vp1, _vp2, _pp)\u001b[0m\n\u001b[0;32m     35\u001b[0m     \u001b[0mvp2W\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_vp2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfocal\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m     \u001b[0mppW\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_pp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 37\u001b[1;33m     \u001b[0mvp3W\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcross\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvp1W\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mppW\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvp2W\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mppW\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     38\u001b[0m     \u001b[0mvp3\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvp3W\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mvp3W\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mfocal\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mppW\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m     \u001b[0mvp3Direction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvp3\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfocal\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mppW\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\yolo\\lib\\site-packages\\numpy\\core\\numeric.py\u001b[0m in \u001b[0;36mcross\u001b[1;34m(a, b, axisa, axisb, axisc, axis)\u001b[0m\n\u001b[0;32m   1827\u001b[0m            \"(dimension must be 2 or 3)\")\n\u001b[0;32m   1828\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1829\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1830\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1831\u001b[0m     \u001b[1;31m# Create the output array\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: incompatible dimensions for cross product\n(dimension must be 2 or 3)"
     ]
    }
   ],
   "source": [
    "tuple_cam = computeCameraCalibration(vp1,vp2,pp)\n",
    "vp1, vp2, vp3, pp, roadPlane, focal = tuple_cam\n",
    "projector = lambda p: getWorldCoordinagesOnRoadPlane(p, focal, roadPlane, pp)\n",
    "points = map(lambda x,y: np.array([x,y,1]), [5,6], [7,8])\n",
    "points = map(projector, points)\n",
    "print(points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = list(points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([5, 6, 1]), array([7, 8, 1])]\n"
     ]
    }
   ],
   "source": [
    "vp1 = [144.737, 34.7794]\n",
    "vp2 = [12183.582175112755, 615.451021479187]\n",
    "pp = [960.5, 540.5]\n",
    "vp1, vp2, vp3, pp, roadPlane, focal = tuple_cam\n",
    "points = map(lambda p: np.array([p[-2],p[-1],1]), ([5,6],[7,8]))\n",
    "print(list(points))\n",
    "# points = map(projector, points)\n",
    "\n",
    "# print(list(points))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First created\n",
      "First deleted\n",
      "1 Counter objects remaining\n"
     ]
    }
   ],
   "source": [
    "from weakref import WeakValueDictionary\n",
    "\n",
    "class Counter:\n",
    "    _instances = WeakValueDictionary()\n",
    "    @property\n",
    "    def Count(self):\n",
    "        return len(self._instances)\n",
    "\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self._instances[id(self)] = self\n",
    "        print (name, 'created')\n",
    "\n",
    "    def __del__(self):\n",
    "        print (self.name, 'deleted')\n",
    "        if self.Count == 0:\n",
    "            print ('Last Counter object deleted')\n",
    "        else:\n",
    "            print (self.Count, 'Counter objects remaining')\n",
    "\n",
    "x = Counter(\"First\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO line 215:22: Added keywords to args of function 'tf.boolean_mask'\n",
      "INFO line 216:27: Added keywords to args of function 'tf.boolean_mask'\n",
      "INFO line 389:23: Added keywords to args of function 'tf.boolean_mask'\n",
      "INFO line 411:19: Renamed 'tf.Print' to 'tf.compat.v1.Print'\n",
      "TensorFlow 2.0 Upgrade Script\n",
      "-----------------------------\n",
      "Converted 1 files\n",
      "Detected 0 issues that require attention\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Make sure to read the detailed log 'report.txt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-10-28 10:39:07.623482: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_100.dll\n"
     ]
    }
   ],
   "source": [
    "!tf_upgrade_v2 --infile yolo_loading.py --outfile yolo_loading-update.py\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "video_path = 'Z:\\\\Vehicle speed measuring\\\\video_speed measure\\\\front.avi'\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "size = (int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)),\n",
    "        int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT)))\n",
    "\n",
    "# Define the codec and create VideoWriter object\n",
    "# fourcc = cv2.CV_FOURCC('m', 'p', '4', 'v') # upper case - yl3\n",
    "fourcc = cv2.VideoWriter_fourcc('M','J','P','G')\n",
    "out = cv2.VideoWriter(\"C:/Users/ADMINS/Desktop/tracking.avi\", fourcc , 25, size, 1) #20.0: number of frames per sec\n",
    "\n",
    "while(cap.isOpened()):\n",
    "    ret, frame = cap.read()\n",
    "    if ret==True:\n",
    "#         frame = cv2.flip(frame,1)\n",
    "\n",
    "        # write the flipped frame\n",
    "        out.write(frame)\n",
    "\n",
    "        cv2.imshow('frame',frame)\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "    else:\n",
    "        break\n",
    "\n",
    "# Release everything if job is finished\n",
    "cap.release()\n",
    "out.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "152899.476\n",
      "45618\n"
     ]
    }
   ],
   "source": [
    "def checkFromTop(p, line):\n",
    "    x,y = p\n",
    "    x1 = 1199\n",
    "    y1 = getY(x1, line)\n",
    "    x2 = 507\n",
    "    y2 = getY(x2, line)\n",
    "    return (x-x1)*(y2-y1)-(y-y1)*(x2-x1) \n",
    "    \n",
    "def getY(x, line):\n",
    "    # ax + by + c =0\n",
    "    a,b,c = line\n",
    "    return (-a*x-c)/b \n",
    "\n",
    "print(checkFromTop([500,550],[-0.376, -1, +517.047]))\n",
    "\n",
    "\n",
    "\n",
    "x,y = [30,450]\n",
    "x1 =   507\n",
    "y1 =    498\n",
    "# b = x-x1 - (y-)\n",
    "x2 =  1199\n",
    "y2 =  472\n",
    "a = (x-x1)*(y2-y1)-(y-y1)*(x2-x1)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "True and False and False"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
